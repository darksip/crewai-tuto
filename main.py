#!/usr/bin/env python3
"""
SystÃ¨me de veille CrewAI - Version Simple
Configuration 100% dÃ©clarative via YAML
"""

import yaml
import json
import argparse
import feedparser
from datetime import datetime, timedelta
from pathlib import Path

# Imports CrewAI
from crewai import Agent, Task, Crew
from crewai_tools import SerperDevTool


def load_config(config_file="veille.yaml"):
    """Charger la configuration YAML"""
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ Fichier {config_file} non trouvÃ©")
        return None
    except yaml.YAMLError as e:
        print(f"âŒ Erreur YAML : {e}")
        return None


def create_agents(config):
    """CrÃ©er les agents depuis la configuration"""
    agents = {}

    for name, agent_config in config["agents"].items():
        agent = Agent(
            role=agent_config["role"],
            goal=agent_config["goal"],
            backstory=agent_config["backstory"],
            verbose=True,
        )
        agents[name] = agent

    return agents


def extract_channel_name(url):
    """Extraire le nom de la chaÃ®ne depuis l'URL YouTube"""
    try:
        if "@" in url:
            # Format: https://www.youtube.com/@Underscore_
            return url.split("@")[-1]
        elif "/c/" in url:
            # Format: https://www.youtube.com/c/Micode
            return url.split("/c/")[-1]
        elif "/channel/" in url:
            # Format: https://www.youtube.com/channel/UCxxx
            return url.split("/channel/")[-1]
        else:
            # Fallback: prendre la derniÃ¨re partie aprÃ¨s /
            return url.split("/")[-1]
    except (IndexError, AttributeError):
        return url  # Retourner l'URL si extraction Ã©choue


def get_channel_id_from_url(channel_url):
    """Obtenir l'ID de la chaÃ®ne depuis son URL YouTube - MÃ©thode curl/grep simple"""
    try:
        # Si c'est dÃ©jÃ  un ID de chaÃ®ne
        if channel_url.startswith("UC") and len(channel_url) == 24:
            return channel_url

        # Si c'est une URL avec /channel/
        if "/channel/" in channel_url:
            return channel_url.split("/channel/")[-1].split("?")[0]

        # Utiliser la mÃ©thode curl/grep qui fonctionne parfaitement
        import subprocess

        cmd = f'''curl -sL "{channel_url}" | grep -oE '("channelId"|"externalId"|"ownerChannelId"):"UC[-_0-9A-Za-z]{{22}}' | head -n1 | grep -oE 'UC[-_0-9A-Za-z]{{22}}' '''

        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, timeout=30
        )

        if result.returncode == 0 and result.stdout.strip():
            channel_id = result.stdout.strip()
            if channel_id.startswith("UC") and len(channel_id) == 24:
                return channel_id

        print(f"âš ï¸ Impossible de trouver l'ID pour {channel_url}")
        return None

    except Exception as e:
        print(f"âŒ Erreur extraction ID chaÃ®ne {channel_url}: {e}")
        return None


def get_recent_videos_from_rss(channel_url, hours_limit=24):
    """RÃ©cupÃ©rer les vidÃ©os rÃ©centes via RSS feed YouTube"""
    try:
        # Obtenir l'ID de la chaÃ®ne
        channel_id = get_channel_id_from_url(channel_url)
        if not channel_id:
            return []

        # Construire l'URL du flux RSS
        rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"

        # Parser le flux RSS
        feed = feedparser.parse(rss_url)

        if not feed.entries:
            print(f"âš ï¸ Aucune vidÃ©o trouvÃ©e dans le flux RSS pour {channel_url}")
            return []

        # Filtrer par date (derniÃ¨res X heures)
        cutoff_time = datetime.now() - timedelta(hours=hours_limit)
        recent_videos = []

        for entry in feed.entries:
            try:
                # Parser la date de publication
                pub_date = datetime(*entry.published_parsed[:6])

                # Garder seulement les vidÃ©os rÃ©centes
                if pub_date > cutoff_time:
                    video = {
                        "title": entry.title,
                        "url": entry.link,
                        "published": entry.published,
                        "channel": feed.feed.title
                        if hasattr(feed.feed, "title")
                        else extract_channel_name(channel_url),
                        "description": getattr(entry, "summary", ""),
                        "published_date": pub_date,
                    }
                    recent_videos.append(video)

            except Exception as e:
                print(f"âš ï¸ Erreur parsing vidÃ©o : {e}")
                continue

        # Trier par date (plus rÃ©cent en premier)
        recent_videos.sort(key=lambda x: x["published_date"], reverse=True)

        return recent_videos

    except Exception as e:
        print(f"âŒ Erreur RSS pour {channel_url} : {e}")
        return []


def create_daily_directory(date):
    """CrÃ©er le rÃ©pertoire daily pour une date donnÃ©e"""
    date_str = date.strftime("%Y-%m-%d")
    daily_dir = Path("daily") / date_str
    daily_dir.mkdir(parents=True, exist_ok=True)
    return daily_dir


def get_processed_videos(daily_dir):
    """RÃ©cupÃ©rer la liste des vidÃ©os dÃ©jÃ  traitÃ©es pour une date"""
    processed_file = daily_dir / "videos_processed.json"

    if processed_file.exists():
        try:
            with open(processed_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data.get("video_ids", []))
        except Exception as e:
            print(f"âš ï¸ Erreur lecture videos_processed.json : {e}")

    return set()


def save_processed_video(daily_dir, video):
    """Sauvegarder une vidÃ©o comme traitÃ©e dans son rÃ©pertoire daily"""
    processed_file = daily_dir / "videos_processed.json"

    # Charger les vidÃ©os dÃ©jÃ  traitÃ©es
    processed_data = {"video_ids": [], "videos": []}
    if processed_file.exists():
        try:
            with open(processed_file, "r", encoding="utf-8") as f:
                processed_data = json.load(f)
        except Exception:
            pass

    # Ajouter la nouvelle vidÃ©o si pas dÃ©jÃ  prÃ©sente
    video_id = (
        video["url"].split("watch?v=")[-1].split("&")[0]
    )  # Extraire l'ID de la vidÃ©o

    if video_id not in processed_data["video_ids"]:
        processed_data["video_ids"].append(video_id)
        processed_data["videos"].append(
            {
                "video_id": video_id,
                "title": video["title"],
                "url": video["url"],
                "channel": video["channel"],
                "published": video["published"],
                "processed_at": datetime.now().isoformat(),
            }
        )

        # Sauvegarder
        try:
            with open(processed_file, "w", encoding="utf-8") as f:
                json.dump(processed_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ Erreur sauvegarde video processÃ©e : {e}")


def get_all_youtube_videos(topic):
    """RÃ©cupÃ©rer toutes les vidÃ©os rÃ©centes pour un topic via RSS (7 jours) et vÃ©rifier dÃ©jÃ  traitÃ©es"""
    all_videos = []
    new_videos_count = 0

    print(f"ğŸ“¡ RÃ©cupÃ©ration RSS pour {topic['name']} (7 derniers jours)...")

    for channel_url in topic["youtube_channels"]:
        channel_name = extract_channel_name(channel_url)
        print(f"  ğŸ“º Analyse de {channel_name}...")

        videos = get_recent_videos_from_rss(
            channel_url, hours_limit=168
        )  # 7 jours = 168 heures

        if videos:
            print(f"    ğŸ“Š {len(videos)} vidÃ©o(s) trouvÃ©e(s) sur 7 jours")

            # Trier par date de publication pour traiter par jour
            for video in videos:
                pub_date = video["published_date"].date()  # Date seulement, sans heure
                daily_dir = create_daily_directory(pub_date)

                # VÃ©rifier si dÃ©jÃ  traitÃ©e
                processed_videos = get_processed_videos(daily_dir)
                video_id = video["url"].split("watch?v=")[-1].split("&")[0]

                if video_id not in processed_videos:
                    video["daily_dir"] = daily_dir
                    video["video_id"] = video_id
                    all_videos.append(video)
                    new_videos_count += 1
                    print(
                        f"    ğŸ†• Nouvelle vidÃ©o pour {pub_date}: {video['title'][:50]}..."
                    )
                else:
                    print(f"    â­ï¸  DÃ©jÃ  traitÃ©e ({pub_date}): {video['title'][:50]}...")
        else:
            print("    âš ï¸ Aucune vidÃ©o trouvÃ©e")

    print(f"ğŸ“ˆ Total : {new_videos_count} nouvelles vidÃ©os Ã  traiter")

    # Trier par date de publication (plus rÃ©cent en premier)
    all_videos.sort(key=lambda x: x["published_date"], reverse=True)
    return all_videos


def create_tasks(config, agents, topic):
    """CrÃ©er les tÃ¢ches pour un topic donnÃ©"""
    tasks = []

    # PrÃ©parer les variables pour le formatage
    variables = {
        "topic_name": topic["name"],
        "keywords": ", ".join(topic["keywords"]),
        "youtube_channels": ", ".join(topic["youtube_channels"]),  # URLs complÃ¨tes
        "volume": topic["volume"],
        "date": datetime.now().strftime("%d/%m/%Y"),
    }

    # CrÃ©er les tÃ¢ches depuis la config
    for task_name, task_config in config["tasks"].items():
        task = Task(
            description=task_config["description"].format(**variables),
            expected_output=task_config["expected_output"],
            agent=agents[task_config["agent"]],
        )
        tasks.append(task)

    return tasks


def save_synthesis(synthesis_content, topic_name, output_dir="syntheses"):
    """Sauvegarder la synthÃ¨se"""
    # CrÃ©er le rÃ©pertoire s'il n'existe pas
    Path(output_dir).mkdir(exist_ok=True)

    # Nom du fichier avec date
    date_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"{output_dir}/synthese_{topic_name.replace(' ', '_')}_{date_str}.md"

    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(synthesis_content)
        print(f"âœ… SynthÃ¨se sauvÃ©e : {filename}")
        return filename
    except Exception as e:
        print(f"âŒ Erreur sauvegarde : {e}")
        return None


def run_veille_for_topic(config, agents, topic):
    """ExÃ©cuter la veille pour un topic avec persistence par date de publication"""
    print(f"\nğŸš€ Traitement du topic : {topic['name']}")

    # Ã‰tape 1: RÃ©cupÃ©rer les nouvelles vidÃ©os (7 jours, vÃ©rification dÃ©jÃ  traitÃ©es)
    new_videos = get_all_youtube_videos(topic)

    if not new_videos:
        print("â„¹ï¸ Aucune nouvelle vidÃ©o Ã  traiter")
        return None

    # Grouper les vidÃ©os par date de publication
    videos_by_date = {}
    for video in new_videos:
        pub_date = video["published_date"].date()
        if pub_date not in videos_by_date:
            videos_by_date[pub_date] = []
        videos_by_date[pub_date].append(video)

    print(f"ğŸ“… VidÃ©os rÃ©parties sur {len(videos_by_date)} jour(s)")

    processed_syntheses = []

    # Traiter chaque jour sÃ©parÃ©ment
    for pub_date, date_videos in sorted(videos_by_date.items(), reverse=True):
        print(f"\nğŸ“† Traitement des vidÃ©os du {pub_date} ({len(date_videos)} vidÃ©os)")

        # PrÃ©parer le contexte pour cette date
        videos_context = f"\n\nVIDÃ‰OS YOUTUBE DU {pub_date} :\n"
        for i, video in enumerate(date_videos, 1):
            videos_context += f"{i}. **{video['title']}** ({video['channel']})\n"
            videos_context += f"   URL: {video['url']}\n"
            videos_context += f"   PubliÃ©: {video['published']}\n"
            if video["description"]:
                videos_context += f"   Description: {video['description'][:100]}...\n"
            videos_context += "\n"

        # CrÃ©er les tÃ¢ches avec contexte spÃ©cifique Ã  cette date
        tasks = create_tasks_with_video_context(
            config, agents, topic, videos_context, pub_date
        )

        # Ajouter l'outil Serper aux agents
        search_tool = SerperDevTool()
        for agent in agents.values():
            agent.tools = [search_tool]

        # CrÃ©er et lancer le crew
        crew = Crew(agents=list(agents.values()), tasks=tasks, verbose=True)

        try:
            print(f"âš¡ Lancement analyse pour {pub_date}...")
            result = crew.kickoff()

            # Sauvegarder la synthÃ¨se dans le rÃ©pertoire daily de la date de publication
            daily_dir = create_daily_directory(pub_date)
            synthesis_file = (
                daily_dir / f"synthese_{topic['name'].replace(' ', '_')}_{pub_date}.md"
            )

            with open(synthesis_file, "w", encoding="utf-8") as f:
                f.write(str(result))

            # Marquer toutes les vidÃ©os de cette date comme traitÃ©es
            for video in date_videos:
                save_processed_video(daily_dir, video)

            print(f"âœ… SynthÃ¨se {pub_date} sauvÃ©e : {synthesis_file}")
            processed_syntheses.append(str(synthesis_file))

        except Exception as e:
            print(f"âŒ Erreur traitement {pub_date} : {e}")

    print(
        f"\nğŸ‰ Traitement terminÃ© : {len(processed_syntheses)} synthÃ¨se(s) gÃ©nÃ©rÃ©e(s)"
    )
    return processed_syntheses


def create_tasks_with_video_context(
    config, agents, topic, videos_context, pub_date=None
):
    """CrÃ©er les tÃ¢ches avec le contexte vidÃ©os prÃ©-rÃ©cupÃ©rÃ© pour une date spÃ©cifique"""
    tasks = []

    # Utiliser la date de publication ou aujourd'hui
    target_date = pub_date if pub_date else datetime.now().date()

    # PrÃ©parer les variables pour le formatage
    variables = {
        "topic_name": topic["name"],
        "keywords": ", ".join(topic["keywords"]),
        "youtube_channels": ", ".join(topic["youtube_channels"]),
        "volume": topic["volume"],
        "date": target_date.strftime("%d/%m/%Y"),
        "videos_context": videos_context,
    }

    # CrÃ©er les tÃ¢ches depuis la config
    for task_name, task_config in config["tasks"].items():
        description = task_config["description"].format(**variables)

        # Pour la tÃ¢che de synthÃ¨se, ajouter le contexte vidÃ©os
        if task_name == "synthesize" and videos_context:
            description += videos_context

        task = Task(
            description=description,
            expected_output=task_config["expected_output"],
            agent=agents[task_config["agent"]],
        )
        tasks.append(task)

    return tasks


def main():
    parser = argparse.ArgumentParser(description="Veille CrewAI Simple")
    parser.add_argument(
        "--config", default="veille.yaml", help="Fichier de configuration"
    )
    parser.add_argument("--topic", help="Topic spÃ©cifique Ã  traiter")
    parser.add_argument("--list-topics", action="store_true", help="Lister les topics")
    parser.add_argument(
        "--test-rss", action="store_true", help="Tester les flux RSS YouTube"
    )
    parser.add_argument(
        "--status-daily",
        action="store_true",
        help="Afficher le statut des rÃ©pertoires daily",
    )
    parser.add_argument("--dry-run", action="store_true", help="Mode simulation")

    args = parser.parse_args()

    # Banner
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            ğŸ¤– CrewAI Veille Simple               â•‘
â•‘                                                  â•‘
â•‘        Configuration 100% dÃ©clarative           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Charger la configuration
    config = load_config(args.config)
    if not config:
        return 1

    # Lister les topics
    if args.list_topics:
        print("ğŸ“‹ Topics configurÃ©s :")
        for topic in config["topics"]:
            # Extraire les noms des chaÃ®nes pour l'affichage
            channel_names = [
                extract_channel_name(url) for url in topic["youtube_channels"]
            ]

            print(f"  â€¢ {topic['name']} (volume: {topic['volume']})")
            print(f"    Mots-clÃ©s : {', '.join(topic['keywords'])}")
            print(f"    ChaÃ®nes YouTube : {', '.join(channel_names)}")
            print()
        return 0

    # Tester les flux RSS YouTube
    if args.test_rss:
        print("ğŸ§ª Test des flux RSS YouTube...")
        print("=" * 50)

        for topic in config["topics"]:
            print(f"\nğŸ“º Topic : {topic['name']}")
            videos = get_all_youtube_videos(topic)

            if videos:
                print(f"âœ… {len(videos)} vidÃ©o(s) rÃ©cente(s) :")
                for video in videos[:3]:  # Afficher les 3 premiÃ¨res
                    print(f"  â€¢ {video['title']}")
                    print(
                        f"    ChaÃ®ne: {video['channel']} | PubliÃ©: {video['published']}"
                    )
                    print(f"    URL: {video['url']}")
                    print()
            else:
                print("âš ï¸ Aucune vidÃ©o rÃ©cente trouvÃ©e")

        return 0

    # Afficher le statut des rÃ©pertoires daily
    if args.status_daily:
        print("ğŸ“Š Statut des rÃ©pertoires daily...")
        print("=" * 50)

        daily_base = Path("daily")
        if not daily_base.exists():
            print("âš ï¸ Aucun rÃ©pertoire daily trouvÃ©")
            return 0

        # Parcourir les rÃ©pertoires de dates
        date_dirs = sorted(
            [d for d in daily_base.iterdir() if d.is_dir()], reverse=True
        )

        if not date_dirs:
            print("âš ï¸ Aucun rÃ©pertoire de date trouvÃ©")
            return 0

        for date_dir in date_dirs[:10]:  # 10 derniers jours
            date_name = date_dir.name
            print(f"\nğŸ“… {date_name}")

            # Compter les vidÃ©os traitÃ©es
            processed_file = date_dir / "videos_processed.json"
            video_count = 0
            if processed_file.exists():
                try:
                    with open(processed_file, "r") as f:
                        data = json.load(f)
                        video_count = len(data.get("video_ids", []))
                except Exception:
                    pass

            # Compter les synthÃ¨ses
            synthesis_files = list(date_dir.glob("synthese_*.md"))

            print(f"  ğŸ“¹ VidÃ©os traitÃ©es : {video_count}")
            print(f"  ğŸ“ SynthÃ¨ses : {len(synthesis_files)}")

            if synthesis_files:
                for synth_file in synthesis_files:
                    print(f"    â€¢ {synth_file.name}")

        return 0

    # Note: Les API keys sont gÃ©rÃ©es par Doppler automatiquement

    if args.dry_run:
        print("ğŸ§ª Mode simulation - Pas d'appels API")
        return 0

    # CrÃ©er les agents
    agents = create_agents(config)
    print(f"ğŸ­ {len(agents)} agents crÃ©Ã©s : {', '.join(agents.keys())}")

    # Traitement des topics
    topics_to_process = config["topics"]

    # Filtrer par topic si spÃ©cifiÃ©
    if args.topic:
        topics_to_process = [
            t for t in topics_to_process if t["name"].lower() == args.topic.lower()
        ]
        if not topics_to_process:
            print(f"âŒ Topic '{args.topic}' non trouvÃ©")
            return 1

    # Traiter chaque topic
    results = []
    for topic in topics_to_process:
        filename = run_veille_for_topic(config, agents, topic)
        if filename:
            results.append(filename)

    # RÃ©sumÃ©
    print("\nğŸ‰ Traitement terminÃ© !")
    print(f"ğŸ“Š {len(results)} synthÃ¨ses gÃ©nÃ©rÃ©es :")
    for filename in results:
        print(f"  ğŸ“„ {filename}")

    return 0


if __name__ == "__main__":
    exit(main())
